{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of chatbot idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_size = \"small\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(model, tokenizer, trained=False):\n",
    "    print(\"type \\\"q\\\" to quit. Automatically quits after 5 messages\")\n",
    "\n",
    "    for step in range(5):\n",
    "        message = input(\"MESSAGE: \")\n",
    "\n",
    "        if message in [\"\", \"q\"]:  # if the user doesn't wanna talk\n",
    "            break\n",
    "\n",
    "        # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "        new_user_input_ids = tokenizer.encode(message + tokenizer.eos_token, return_tensors='pt') \n",
    "\n",
    "        # append the new user input tokens to the chat history\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "        # generated a response while limiting the total chat history to 1000 tokens, \n",
    "        if (trained):\n",
    "            chat_history_ids = model.generate(\n",
    "                bot_input_ids, \n",
    "                max_length=1000,\n",
    "                pad_token_id=tokenizer.eos_token_id,  \n",
    "                no_repeat_ngram_size=3,       \n",
    "                do_sample=True, \n",
    "                top_k=100, \n",
    "                top_p=0.7,\n",
    "                temperature = 0.8, \n",
    "            )\n",
    "        else:\n",
    "            chat_history_ids = model.generate(\n",
    "                bot_input_ids, \n",
    "                max_length=1000, \n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                no_repeat_ngram_size=3\n",
    "            )\n",
    "\n",
    "        # pretty print last ouput tokens from bot\n",
    "        print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "\n",
    "#chat(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, logging, os, pickle, random, re, torch, pandas as pd, numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Args to allow for easy convertion of python script to notebook\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = f'output-{model_size}'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = f'microsoft/DialoGPT-{model_size}'\n",
    "        self.config_name = f'microsoft/DialoGPT-{model_size}'\n",
    "        self.tokenizer_name = f'microsoft/DialoGPT-{model_size}'\n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 512\n",
    "        self.per_gpu_train_batch_size = 1\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 40  # 3\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 1000\n",
    "        self.save_total_limit = None\n",
    "        self.seed = 42\n",
    "        self.local_rank = -1\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_name = 'kafka-the-metamorphosis'\n",
    "df = pd.read_csv(f\"./input/processed_books/{book_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context1</th>\n",
       "      <th>context2</th>\n",
       "      <th>context3</th>\n",
       "      <th>context4</th>\n",
       "      <th>context5</th>\n",
       "      <th>context6</th>\n",
       "      <th>context7</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It was a picture of a woman with a fur hat an...</td>\n",
       "      <td>She sat erect there, lifting up in the direct...</td>\n",
       "      <td>Gregor’s glance then turned to the window</td>\n",
       "      <td>The dreary weather (the rain drops were falli...</td>\n",
       "      <td>‘Why don’t I keep sleeping for a little while...</td>\n",
       "      <td>But this was entirely impractical, for he was...</td>\n",
       "      <td>No matter how hard he threw himself onto his ...</td>\n",
       "      <td>She sat erect there, lifting up in the direct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She sat erect there, lifting up in the direct...</td>\n",
       "      <td>Gregor’s glance then turned to the window</td>\n",
       "      <td>The dreary weather (the rain drops were falli...</td>\n",
       "      <td>‘Why don’t I keep sleeping for a little while...</td>\n",
       "      <td>But this was entirely impractical, for he was...</td>\n",
       "      <td>No matter how hard he threw himself onto his ...</td>\n",
       "      <td>He must have tried it a hundred times, closin...</td>\n",
       "      <td>Gregor’s glance then turned to the window</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gregor’s glance then turned to the window</td>\n",
       "      <td>The dreary weather (the rain drops were falli...</td>\n",
       "      <td>‘Why don’t I keep sleeping for a little while...</td>\n",
       "      <td>But this was entirely impractical, for he was...</td>\n",
       "      <td>No matter how hard he threw himself onto his ...</td>\n",
       "      <td>He must have tried it a hundred times, closin...</td>\n",
       "      <td>‘O God,’ he thought, ‘what a demanding job I’v...</td>\n",
       "      <td>The dreary weather (the rain drops were falli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The dreary weather (the rain drops were falli...</td>\n",
       "      <td>‘Why don’t I keep sleeping for a little while...</td>\n",
       "      <td>But this was entirely impractical, for he was...</td>\n",
       "      <td>No matter how hard he threw himself onto his ...</td>\n",
       "      <td>He must have tried it a hundred times, closin...</td>\n",
       "      <td>‘O God,’ he thought, ‘what a demanding job I’v...</td>\n",
       "      <td>The stresses of trade are much greater than t...</td>\n",
       "      <td>‘Why don’t I keep sleeping for a little while...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‘Why don’t I keep sleeping for a little while...</td>\n",
       "      <td>But this was entirely impractical, for he was...</td>\n",
       "      <td>No matter how hard he threw himself onto his ...</td>\n",
       "      <td>He must have tried it a hundred times, closin...</td>\n",
       "      <td>‘O God,’ he thought, ‘what a demanding job I’v...</td>\n",
       "      <td>The stresses of trade are much greater than t...</td>\n",
       "      <td>To hell with it all!’ He felt a slight itchin...</td>\n",
       "      <td>But this was entirely impractical, for he was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            context1  \\\n",
       "0   It was a picture of a woman with a fur hat an...   \n",
       "1   She sat erect there, lifting up in the direct...   \n",
       "2          Gregor’s glance then turned to the window   \n",
       "3   The dreary weather (the rain drops were falli...   \n",
       "4   ‘Why don’t I keep sleeping for a little while...   \n",
       "\n",
       "                                            context2  \\\n",
       "0   She sat erect there, lifting up in the direct...   \n",
       "1          Gregor’s glance then turned to the window   \n",
       "2   The dreary weather (the rain drops were falli...   \n",
       "3   ‘Why don’t I keep sleeping for a little while...   \n",
       "4   But this was entirely impractical, for he was...   \n",
       "\n",
       "                                            context3  \\\n",
       "0          Gregor’s glance then turned to the window   \n",
       "1   The dreary weather (the rain drops were falli...   \n",
       "2   ‘Why don’t I keep sleeping for a little while...   \n",
       "3   But this was entirely impractical, for he was...   \n",
       "4   No matter how hard he threw himself onto his ...   \n",
       "\n",
       "                                            context4  \\\n",
       "0   The dreary weather (the rain drops were falli...   \n",
       "1   ‘Why don’t I keep sleeping for a little while...   \n",
       "2   But this was entirely impractical, for he was...   \n",
       "3   No matter how hard he threw himself onto his ...   \n",
       "4   He must have tried it a hundred times, closin...   \n",
       "\n",
       "                                            context5  \\\n",
       "0   ‘Why don’t I keep sleeping for a little while...   \n",
       "1   But this was entirely impractical, for he was...   \n",
       "2   No matter how hard he threw himself onto his ...   \n",
       "3   He must have tried it a hundred times, closin...   \n",
       "4  ‘O God,’ he thought, ‘what a demanding job I’v...   \n",
       "\n",
       "                                            context6  \\\n",
       "0   But this was entirely impractical, for he was...   \n",
       "1   No matter how hard he threw himself onto his ...   \n",
       "2   He must have tried it a hundred times, closin...   \n",
       "3  ‘O God,’ he thought, ‘what a demanding job I’v...   \n",
       "4   The stresses of trade are much greater than t...   \n",
       "\n",
       "                                            context7  \\\n",
       "0   No matter how hard he threw himself onto his ...   \n",
       "1   He must have tried it a hundred times, closin...   \n",
       "2  ‘O God,’ he thought, ‘what a demanding job I’v...   \n",
       "3   The stresses of trade are much greater than t...   \n",
       "4   To hell with it all!’ He felt a slight itchin...   \n",
       "\n",
       "                                            response  \n",
       "0   She sat erect there, lifting up in the direct...  \n",
       "1          Gregor’s glance then turned to the window  \n",
       "2   The dreary weather (the rain drops were falli...  \n",
       "3   ‘Why don’t I keep sleeping for a little while...  \n",
       "4   But this was entirely impractical, for he was...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, df_trn):\n",
    "    return ConversationDataset(tokenizer, args, df_trn)\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(directory, args.model_type + \"_cached_lm_\" + str(block_size))\n",
    "\n",
    "        logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "        self.examples = []\n",
    "        for _, row in df.iterrows():\n",
    "            conv = construct_conv(row, tokenizer)\n",
    "            self.examples.append(conv)\n",
    "\n",
    "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "        with open(cached_features_file, \"wb\") as handle:\n",
    "            pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    logger.info(\"*** Running trainng, Num examples = %d, Num Epochs = %d ***\", len(train_dataset), args.num_train_epochs)\n",
    "\n",
    "    global_step, epochs_trained = 0, 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            \n",
    "            inputs, labels = (batch, batch)\n",
    "            if inputs.shape[1] > 1024: continue\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "    tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df_trn):\n",
    "    args = Args()\n",
    "    \n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    device = torch.device(\"cuda\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s\", args.local_rank, device, args.n_gpu)\n",
    "\n",
    "    set_seed(args) # Set seed\n",
    "\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, from_tf=False, config=config, cache_dir=args.cache_dir)\n",
    "    model.to(args.device)\n",
    "    \n",
    "    # Training\n",
    "    train_dataset = load_and_cache_examples(args, tokenizer, df_trn)\n",
    "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "    model_to_save = (model.module if hasattr(model, \"module\") else model)  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(args.output_dir)\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "    # Good practice: save your training arguments together with the trained model\n",
    "    torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "    # Load a trained model and vocabulary that you have fine-tuned\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "    model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/14/2023 17:22:14 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1\n",
      "05/14/2023 17:22:23 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "05/14/2023 17:22:26 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "c:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "05/14/2023 17:22:26 - INFO - __main__ -   *** Running trainng, Num examples = 876, Num Epochs = 40 ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77040a16b6da475aad8ff1268cb56505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a6d97c20ac4d198e6d8c97b3642ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 136.00 MiB (GPU 0; 4.00 GiB total capacity; 3.13 GiB already allocated; 0 bytes free; 3.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m main(df)\n",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(df_trn)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m# Training\u001b[39;00m\n\u001b[0;32m     25\u001b[0m train_dataset \u001b[39m=\u001b[39m load_and_cache_examples(args, tokenizer, df_trn)\n\u001b[1;32m---> 26\u001b[0m global_step, tr_loss \u001b[39m=\u001b[39m train(args, train_dataset, model, tokenizer)\n\u001b[0;32m     27\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39m global_step = \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, average loss = \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, global_step, tr_loss)\n\u001b[0;32m     29\u001b[0m \u001b[39m# Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 60\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, train_dataset, model, tokenizer)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     58\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m---> 60\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     62\u001b[0m tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 0; 4.00 GiB total capacity; 3.13 GiB already allocated; 0 bytes free; 3.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "main(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearningEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
