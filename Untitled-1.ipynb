{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of chatbot idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model_size \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msmall\u001b[39m\u001b[39m\"\u001b[39m \n\u001b[0;32m      6\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmicrosoft/DialoGPT-\u001b[39m\u001b[39m{\u001b[39;00mmodel_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmicrosoft/DialoGPT-\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_size\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:463\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    462\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    464\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    465\u001b[0m     )\n\u001b[0;32m    466\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    467\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\transformers\\modeling_utils.py:2156\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2153\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[0;32m   2155\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m-> 2156\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(config, \u001b[39m*\u001b[39mmodel_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2158\u001b[0m \u001b[39mif\u001b[39;00m load_in_8bit:\n\u001b[0;32m   2159\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbitsandbytes\u001b[39;00m \u001b[39mimport\u001b[39;00m get_key_to_not_convert, replace_8bit_linear\n",
      "File \u001b[1;32mc:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:967\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[0;32m    966\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[1;32m--> 967\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m GPT2Model(config)\n\u001b[0;32m    968\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mn_embd, config\u001b[39m.\u001b[39mvocab_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    970\u001b[0m     \u001b[39m# Model parallel\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:696\u001b[0m, in \u001b[0;36mGPT2Model.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mmax_position_embeddings, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[0;32m    695\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39membd_pdrop)\n\u001b[1;32m--> 696\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([GPT2Block(config, layer_idx\u001b[39m=\u001b[39mi) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    697\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_f \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m    699\u001b[0m \u001b[39m# Model parallel\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:696\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mmax_position_embeddings, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[0;32m    695\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39membd_pdrop)\n\u001b[1;32m--> 696\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([GPT2Block(config, layer_idx\u001b[39m=\u001b[39;49mi) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    697\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_f \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m    699\u001b[0m \u001b[39m# Model parallel\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:384\u001b[0m, in \u001b[0;36mGPT2Block.__init__\u001b[1;34m(self, config, layer_idx)\u001b[0m\n\u001b[0;32m    381\u001b[0m inner_dim \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mn_inner \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mn_inner \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m4\u001b[39m \u001b[39m*\u001b[39m hidden_size\n\u001b[0;32m    383\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(hidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m--> 384\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn \u001b[39m=\u001b[39m GPT2Attention(config, layer_idx\u001b[39m=\u001b[39;49mlayer_idx)\n\u001b[0;32m    385\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(hidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39madd_cross_attention:\n",
      "File \u001b[1;32mc:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:170\u001b[0m, in \u001b[0;36mGPT2Attention.__init__\u001b[1;34m(self, config, is_cross_attention, layer_idx)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_attn \u001b[39m=\u001b[39m Conv1D(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[0;32m    169\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_attn \u001b[39m=\u001b[39m Conv1D(\u001b[39m3\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim)\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj \u001b[39m=\u001b[39m Conv1D(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[0;32m    173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_dropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39mattn_pdrop)\n",
      "File \u001b[1;32mc:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\transformers\\pytorch_utils.py:107\u001b[0m, in \u001b[0;36mConv1D.__init__\u001b[1;34m(self, nf, nx)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf \u001b[39m=\u001b[39m nf\n\u001b[0;32m    106\u001b[0m w \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mempty(nx, nf)\n\u001b[1;32m--> 107\u001b[0m nn\u001b[39m.\u001b[39;49minit\u001b[39m.\u001b[39;49mnormal_(w, std\u001b[39m=\u001b[39;49m\u001b[39m0.02\u001b[39;49m)\n\u001b[0;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(w)\n\u001b[0;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mzeros(nf))\n",
      "File \u001b[1;32mc:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\torch\\nn\\init.py:155\u001b[0m, in \u001b[0;36mnormal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[0;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhandle_torch_function(normal_, (tensor,), tensor\u001b[39m=\u001b[39mtensor, mean\u001b[39m=\u001b[39mmean, std\u001b[39m=\u001b[39mstd)\n\u001b[1;32m--> 155\u001b[0m \u001b[39mreturn\u001b[39;00m _no_grad_normal_(tensor, mean, std)\n",
      "File \u001b[1;32mc:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\torch\\nn\\init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[0;32m     18\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 19\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mnormal_(mean, std)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_size = \"small\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(model, tokenizer, trained=False):\n",
    "    print(\"type \\\"q\\\" to quit. Automatically quits after 5 messages\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for step in range(5):\n",
    "        message = input(\"MESSAGE: \")\n",
    "        print(\"You: {}\".format(message))\n",
    "\n",
    "        if message in [\"\", \"q\"]:  # if the user doesn't wanna talk\n",
    "            break\n",
    "\n",
    "        # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "        new_user_input_ids = tokenizer.encode(message + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "\n",
    "        # append the new user input tokens to the chat history\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "        # generated a response while limiting the total chat history to 1000 tokens, \n",
    "        if (trained):\n",
    "            chat_history_ids = model.generate(\n",
    "                bot_input_ids, \n",
    "                max_length=1000,\n",
    "                pad_token_id=tokenizer.eos_token_id,  \n",
    "                no_repeat_ngram_size=3,       \n",
    "                do_sample=True, \n",
    "                top_k=100, \n",
    "                top_p=0.7,\n",
    "                temperature = 0.8, \n",
    "            )\n",
    "        else:\n",
    "            chat_history_ids = model.generate(\n",
    "                bot_input_ids, \n",
    "                max_length=1000, \n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                no_repeat_ngram_size=3\n",
    "            )\n",
    "\n",
    "        # pretty print last ouput tokens from bot\n",
    "        print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "\n",
    "#chat(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, logging, os, pickle, random, re, torch, pandas as pd, numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Args to allow for easy convertion of python script to notebook\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = f'output-{model_size}'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = f'microsoft/DialoGPT-{model_size}'\n",
    "        self.config_name = f'microsoft/DialoGPT-{model_size}'\n",
    "        self.tokenizer_name = f'microsoft/DialoGPT-{model_size}'\n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 512\n",
    "        self.per_gpu_train_batch_size = 1\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 40  # 3\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 1000\n",
    "        self.save_total_limit = None\n",
    "        self.seed = 42\n",
    "        self.local_rank = -1\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "book_name = 'kafka-the-metamorphosis'\n",
    "df = pd.read_csv(f\"./input/processed_books/{book_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context1</th>\n",
       "      <th>context2</th>\n",
       "      <th>context3</th>\n",
       "      <th>context4</th>\n",
       "      <th>context5</th>\n",
       "      <th>context6</th>\n",
       "      <th>context7</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It was a picture of a woman with a fur hat an...</td>\n",
       "      <td>She sat erect there, lifting up in the direct...</td>\n",
       "      <td>Gregor’s glance then turned to the window</td>\n",
       "      <td>The dreary weather (the rain drops were falli...</td>\n",
       "      <td>‘Why don’t I keep sleeping for a little while...</td>\n",
       "      <td>But this was entirely impractical, for he was...</td>\n",
       "      <td>No matter how hard he threw himself onto his ...</td>\n",
       "      <td>She sat erect there, lifting up in the direct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She sat erect there, lifting up in the direct...</td>\n",
       "      <td>Gregor’s glance then turned to the window</td>\n",
       "      <td>The dreary weather (the rain drops were falli...</td>\n",
       "      <td>‘Why don’t I keep sleeping for a little while...</td>\n",
       "      <td>But this was entirely impractical, for he was...</td>\n",
       "      <td>No matter how hard he threw himself onto his ...</td>\n",
       "      <td>He must have tried it a hundred times, closin...</td>\n",
       "      <td>Gregor’s glance then turned to the window</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gregor’s glance then turned to the window</td>\n",
       "      <td>The dreary weather (the rain drops were falli...</td>\n",
       "      <td>‘Why don’t I keep sleeping for a little while...</td>\n",
       "      <td>But this was entirely impractical, for he was...</td>\n",
       "      <td>No matter how hard he threw himself onto his ...</td>\n",
       "      <td>He must have tried it a hundred times, closin...</td>\n",
       "      <td>‘O God,’ he thought, ‘what a demanding job I’v...</td>\n",
       "      <td>The dreary weather (the rain drops were falli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The dreary weather (the rain drops were falli...</td>\n",
       "      <td>‘Why don’t I keep sleeping for a little while...</td>\n",
       "      <td>But this was entirely impractical, for he was...</td>\n",
       "      <td>No matter how hard he threw himself onto his ...</td>\n",
       "      <td>He must have tried it a hundred times, closin...</td>\n",
       "      <td>‘O God,’ he thought, ‘what a demanding job I’v...</td>\n",
       "      <td>The stresses of trade are much greater than t...</td>\n",
       "      <td>‘Why don’t I keep sleeping for a little while...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‘Why don’t I keep sleeping for a little while...</td>\n",
       "      <td>But this was entirely impractical, for he was...</td>\n",
       "      <td>No matter how hard he threw himself onto his ...</td>\n",
       "      <td>He must have tried it a hundred times, closin...</td>\n",
       "      <td>‘O God,’ he thought, ‘what a demanding job I’v...</td>\n",
       "      <td>The stresses of trade are much greater than t...</td>\n",
       "      <td>To hell with it all!’ He felt a slight itchin...</td>\n",
       "      <td>But this was entirely impractical, for he was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            context1  \\\n",
       "0   It was a picture of a woman with a fur hat an...   \n",
       "1   She sat erect there, lifting up in the direct...   \n",
       "2          Gregor’s glance then turned to the window   \n",
       "3   The dreary weather (the rain drops were falli...   \n",
       "4   ‘Why don’t I keep sleeping for a little while...   \n",
       "\n",
       "                                            context2  \\\n",
       "0   She sat erect there, lifting up in the direct...   \n",
       "1          Gregor’s glance then turned to the window   \n",
       "2   The dreary weather (the rain drops were falli...   \n",
       "3   ‘Why don’t I keep sleeping for a little while...   \n",
       "4   But this was entirely impractical, for he was...   \n",
       "\n",
       "                                            context3  \\\n",
       "0          Gregor’s glance then turned to the window   \n",
       "1   The dreary weather (the rain drops were falli...   \n",
       "2   ‘Why don’t I keep sleeping for a little while...   \n",
       "3   But this was entirely impractical, for he was...   \n",
       "4   No matter how hard he threw himself onto his ...   \n",
       "\n",
       "                                            context4  \\\n",
       "0   The dreary weather (the rain drops were falli...   \n",
       "1   ‘Why don’t I keep sleeping for a little while...   \n",
       "2   But this was entirely impractical, for he was...   \n",
       "3   No matter how hard he threw himself onto his ...   \n",
       "4   He must have tried it a hundred times, closin...   \n",
       "\n",
       "                                            context5  \\\n",
       "0   ‘Why don’t I keep sleeping for a little while...   \n",
       "1   But this was entirely impractical, for he was...   \n",
       "2   No matter how hard he threw himself onto his ...   \n",
       "3   He must have tried it a hundred times, closin...   \n",
       "4  ‘O God,’ he thought, ‘what a demanding job I’v...   \n",
       "\n",
       "                                            context6  \\\n",
       "0   But this was entirely impractical, for he was...   \n",
       "1   No matter how hard he threw himself onto his ...   \n",
       "2   He must have tried it a hundred times, closin...   \n",
       "3  ‘O God,’ he thought, ‘what a demanding job I’v...   \n",
       "4   The stresses of trade are much greater than t...   \n",
       "\n",
       "                                            context7  \\\n",
       "0   No matter how hard he threw himself onto his ...   \n",
       "1   He must have tried it a hundred times, closin...   \n",
       "2  ‘O God,’ he thought, ‘what a demanding job I’v...   \n",
       "3   The stresses of trade are much greater than t...   \n",
       "4   To hell with it all!’ He felt a slight itchin...   \n",
       "\n",
       "                                            response  \n",
       "0   She sat erect there, lifting up in the direct...  \n",
       "1          Gregor’s glance then turned to the window  \n",
       "2   The dreary weather (the rain drops were falli...  \n",
       "3   ‘Why don’t I keep sleeping for a little while...  \n",
       "4   But this was entirely impractical, for he was...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, df_trn):\n",
    "    return ConversationDataset(tokenizer, args, df_trn)\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(directory, args.model_type + \"_cached_lm_\" + str(block_size))\n",
    "\n",
    "        logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "        self.examples = []\n",
    "        for _, row in df.iterrows():\n",
    "            conv = construct_conv(row, tokenizer)\n",
    "            self.examples.append(conv)\n",
    "\n",
    "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "        with open(cached_features_file, \"wb\") as handle:\n",
    "            pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    logger.info(\"*** Running trainng, Num examples = %d, Num Epochs = %d ***\", len(train_dataset), args.num_train_epochs)\n",
    "\n",
    "    global_step, epochs_trained = 0, 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            \n",
    "            inputs, labels = (batch, batch)\n",
    "            if inputs.shape[1] > 1024: continue\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "    tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df_trn):\n",
    "    args = Args()\n",
    "    \n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    device = torch.device(\"cuda\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s\", args.local_rank, device, args.n_gpu)\n",
    "\n",
    "    set_seed(args) # Set seed\n",
    "\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, from_tf=False, config=config, cache_dir=args.cache_dir)\n",
    "    model.to(args.device)\n",
    "    \n",
    "    # Training\n",
    "    train_dataset = load_and_cache_examples(args, tokenizer, df_trn)\n",
    "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "    model_to_save = (model.module if hasattr(model, \"module\") else model)  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(args.output_dir)\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "    # Good practice: save your training arguments together with the trained model\n",
    "    torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "    # Load a trained model and vocabulary that you have fine-tuned\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "    model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/14/2023 17:22:14 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1\n",
      "05/14/2023 17:22:23 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "05/14/2023 17:22:26 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "c:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "05/14/2023 17:22:26 - INFO - __main__ -   *** Running trainng, Num examples = 876, Num Epochs = 40 ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77040a16b6da475aad8ff1268cb56505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a6d97c20ac4d198e6d8c97b3642ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 136.00 MiB (GPU 0; 4.00 GiB total capacity; 3.13 GiB already allocated; 0 bytes free; 3.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m main(df)\n",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(df_trn)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m# Training\u001b[39;00m\n\u001b[0;32m     25\u001b[0m train_dataset \u001b[39m=\u001b[39m load_and_cache_examples(args, tokenizer, df_trn)\n\u001b[1;32m---> 26\u001b[0m global_step, tr_loss \u001b[39m=\u001b[39m train(args, train_dataset, model, tokenizer)\n\u001b[0;32m     27\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39m global_step = \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, average loss = \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, global_step, tr_loss)\n\u001b[0;32m     29\u001b[0m \u001b[39m# Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 60\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, train_dataset, model, tokenizer)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     58\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m---> 60\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     62\u001b[0m tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ismyn\\miniconda3\\envs\\chatter\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 0; 4.00 GiB total capacity; 3.13 GiB already allocated; 0 bytes free; 3.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "main(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from main function\n",
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "output_dir = 'output-small'\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type \"q\" to quit. Automatically quits after 5 messages\n",
      "You: HI, How are you?\n",
      "DialoGPT:  ‘All right, then we’ll go,’ he said and looked up at Mr\n",
      "You: Where do we go?\n",
      "DialoGPT:  Traveling is exhausting, but I couldn’t live without it\n",
      "You: I hate you\n",
      "DialoGPT:  I see it coming\n",
      "You: What?\n",
      "DialoGPT: !!!’!! Reminds me of the great hurry I just made,‘I’m almost angry that I didn’ve got away from work\n",
      "You: Why are you so full of anger?\n",
      "DialoGPT:  Why have I not reported that to the office! But people always think that they’d earn pots of money and thus lead a fine life\n"
     ]
    }
   ],
   "source": [
    "chat(model, tokenizer, trained=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearningEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
