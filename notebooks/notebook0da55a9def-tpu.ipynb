{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exploration of chatbot idea"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, DistributedSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "GITHUB_TOKEN = \"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\"\n",
    "USER = \"ja-nina\"\n",
    "CLONE_URL = f\"https://{USER}:{GITHUB_TOKEN}@github.com/{USER}/chatbot.git\"\n",
    "get_ipython().system(f\"git clone {CLONE_URL}\")\n",
    "sys.path.append(\"chatbot\")\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_size = \"small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def chat(model, tokenizer, trained=False):\n",
    "    print(\"type \\\"q\\\" to quit. Automatically quits after 5 messages\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    chat_history_ids = None\n",
    "    for step in range(5):\n",
    "        message = input(\"MESSAGE: \")\n",
    "        print(\"You: {}\".format(message))\n",
    "\n",
    "        if message in [\"\", \"q\"]:  # if the user doesn't wanna talk\n",
    "            break\n",
    "\n",
    "        # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "        new_user_input_ids = tokenizer.encode(message + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "\n",
    "        # append the new user input tokens to the chat history\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "        # generated a response while limiting the total chat history to 1000 tokens,\n",
    "        if (trained):\n",
    "            chat_history_ids = model.generate(\n",
    "                bot_input_ids,\n",
    "                max_length=1000,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                no_repeat_ngram_size=3,\n",
    "                do_sample=True,\n",
    "                top_k=100,\n",
    "                top_p=0.7,\n",
    "                temperature=0.8,\n",
    "            )\n",
    "        else:\n",
    "            chat_history_ids = model.generate(\n",
    "                bot_input_ids,\n",
    "                max_length=1000,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                no_repeat_ngram_size=3\n",
    "            )\n",
    "\n",
    "        # pretty print last ouput tokens from bot\n",
    "        print(\"DialoGPT: {}\".format(\n",
    "            tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "\n",
    "# chat(model, tokenizer)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Args to allow for easy conversion of python script to notebook\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.output_dir = f'output-{model_size}'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = f'microsoft/DialoGPT-{model_size}'\n",
    "        self.config_name = f'microsoft/DialoGPT-{model_size}'\n",
    "        self.tokenizer_name = f'microsoft/DialoGPT-{model_size}'\n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 512\n",
    "        self.per_gpu_train_batch_size = 2\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 1e-4\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 10  # 3\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 1000\n",
    "        self.save_total_limit = None\n",
    "        self.seed = 42\n",
    "        self.local_rank = -1\n",
    "\n",
    "\n",
    "args = Args()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.device_count()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "book_names = ['ferdydurke', 'gombrowicz diary', 'gombrowicz diary_2', 'gombrowicz diary_3', 'gombrowicz-cosmospdf']\n",
    "\n",
    "df = pd.concat([pd.read_csv(f\"/kaggle/working/chatbot/input/processed_books/{book_name}.csv\", sep=\";\") for book_name in\n",
    "                book_names])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.head()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def flatten(item_list):\n",
    "    return [item for sublist in item_list for item in sublist]\n",
    "\n",
    "\n",
    "def construct_conv(row, tokenizer):\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, df_trn):\n",
    "    return ConversationDataset(tokenizer, args, df_trn)\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(directory, args.model_type + \"_cached_lm_\" + str(block_size))\n",
    "\n",
    "        logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "        self.examples = []\n",
    "        for _, row in df.iterrows():\n",
    "            conv = construct_conv(row, tokenizer)\n",
    "            self.examples.append(conv)\n",
    "\n",
    "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "        with open(cached_features_file, \"wb\") as handle:\n",
    "            pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last=True\n",
    "    )\n",
    "\n",
    "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    logger.info(\"*** Running trainng, Num examples = %d, Num Epochs = %d ***\", len(train_dataset),\n",
    "                args.num_train_epochs)\n",
    "\n",
    "    global_step, epochs_trained = 0, 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        logger.warning(\"Epoch done!\")\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            inputs, labels = (batch, batch)\n",
    "            if inputs.shape[1] > 1024:\n",
    "                continue\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "    tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def main(df_trn):\n",
    "    args = Args()\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    device = torch.device(\"cuda\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s\", args.local_rank, device, args.n_gpu)\n",
    "\n",
    "    set_seed(args)  # Set seed\n",
    "\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, from_tf=False, config=config,\n",
    "                                                 cache_dir=args.cache_dir)\n",
    "    model.to(args.device)\n",
    "\n",
    "    # Training\n",
    "    train_dataset = load_and_cache_examples(args, tokenizer, df_trn)\n",
    "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer,\n",
    "    # you can reload them using from_pretrained()\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "    model_to_save = (model.module if hasattr(model, \"module\") else model)  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(args.output_dir)\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "    # Good practice: save your training arguments together with the trained model\n",
    "    torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "    # Load a trained model and vocabulary that you have fine-tuned\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "    model.to(args.device)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "main(df)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Working with pretrained model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# code from main function\n",
    "device = torch.device(\"cuda\")\n",
    "output_dir = 'output-small'\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model.to(device)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "chat(model, tokenizer, trained=True)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}