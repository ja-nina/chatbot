{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploration of chatbot idea","metadata":{}},{"cell_type":"code","source":"# import os\n\n# def remove_folder_contents(folder):\n#     for the_file in os.listdir(folder):\n#         file_path = os.path.join(folder, the_file)\n#         try:\n#             if os.path.isfile(file_path):\n#                 os.unlink(file_path)\n#             elif os.path.isdir(file_path):\n#                 remove_folder_contents(file_path)\n#                 os.rmdir(file_path)\n#         except Exception as e:\n#             print(e)\n\n# folder_path = '/kaggle/working'\n# remove_folder_contents('chatbot')\n# os.rmdir(folder_path)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T12:32:47.706206Z","iopub.execute_input":"2023-05-22T12:32:47.706559Z","iopub.status.idle":"2023-05-22T12:32:47.714329Z","shell.execute_reply.started":"2023-05-22T12:32:47.706525Z","shell.execute_reply":"2023-05-22T12:32:47.713425Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-05-22T12:32:47.716821Z","iopub.execute_input":"2023-05-22T12:32:47.717470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GITHUB_TOKEN = \"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\"\nUSER = \"ja-nina\"\nCLONE_URL = f\"https://{USER}:{GITHUB_TOKEN}@github.com/{USER}/chatbot.git\"\nget_ipython().system(f\"git clone {CLONE_URL}\")\n\nimport sys\nsys.path.append(\"chatbot\")\n\nimport chatbot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_size = \"small\" \n\ntokenizer = AutoTokenizer.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")\nmodel = AutoModelForCausalLM.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chat(model, tokenizer, trained=False):\n    print(\"type \\\"q\\\" to quit. Automatically quits after 5 messages\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    for step in range(5):\n        message = input(\"MESSAGE: \")\n        print(\"You: {}\".format(message))\n\n        if message in [\"\", \"q\"]:  # if the user doesn't wanna talk\n            break\n\n        # encode the new user input, add the eos_token and return a tensor in Pytorch\n        new_user_input_ids = tokenizer.encode(message + tokenizer.eos_token, return_tensors='pt').to(device)\n\n        # append the new user input tokens to the chat history\n        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n        # generated a response while limiting the total chat history to 1000 tokens, \n        if (trained):\n            chat_history_ids = model.generate(\n                bot_input_ids, \n                max_length=1000,\n                pad_token_id=tokenizer.eos_token_id,  \n                no_repeat_ngram_size=3,       \n                do_sample=True, \n                top_k=100, \n                top_p=0.7,\n                temperature = 0.8, \n            )\n        else:\n            chat_history_ids = model.generate(\n                bot_input_ids, \n                max_length=1000, \n                pad_token_id=tokenizer.eos_token_id,\n                no_repeat_ngram_size=3\n            )\n\n        # pretty print last ouput tokens from bot\n        print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n\n#chat(model, tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob, logging, os, pickle, random, re, torch, pandas as pd, numpy as np\nfrom typing import Dict, List, Tuple\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom tqdm.notebook import tqdm, trange\nfrom pathlib import Path\nfrom transformers import (\n    AdamW,\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    get_linear_schedule_with_warmup,\n)\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    from tensorboardX import SummaryWriter\n\nlogger = logging.getLogger(__name__)\n\n# Args to allow for easy convertion of python script to notebook\nclass Args():\n    def __init__(self):\n        self.output_dir = f'output-{model_size}'\n        self.model_type = 'gpt2'\n        self.model_name_or_path = f'microsoft/DialoGPT-{model_size}'\n        self.config_name = f'microsoft/DialoGPT-{model_size}'\n        self.tokenizer_name = f'microsoft/DialoGPT-{model_size}'\n        self.cache_dir = 'cached'\n        self.block_size = 512\n        self.per_gpu_train_batch_size = 2\n        self.gradient_accumulation_steps = 1\n        self.learning_rate = 1e-4\n        self.weight_decay = 0.0\n        self.adam_epsilon = 1e-8\n        self.max_grad_norm = 1.0\n        self.num_train_epochs = 10  # 3\n        self.max_steps = -1\n        self.warmup_steps = 0\n        self.logging_steps = 1000\n        self.save_total_limit = None\n        self.seed = 42\n        self.local_rank = -1\n\nargs = Args()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.device_count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nbook_names = ['ferdydurke','gombrowicz diary', 'gombrowicz diary_2','gombrowicz diary_3', 'gombrowicz-cosmospdf']\n\ndf = pd.concat([pd.read_csv(f\"/kaggle/working/chatbot/input/processed_books/{book_name}.csv\", sep = \";\") for book_name in book_names])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def construct_conv(row, tokenizer, eos = True):\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n    conv = flatten(conv)\n    return conv\n\ndef load_and_cache_examples(args, tokenizer, df_trn):\n    return ConversationDataset(tokenizer, args, df_trn)\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\nclass ConversationDataset(Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n\n        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n        directory = args.cache_dir\n        cached_features_file = os.path.join(directory, args.model_type + \"_cached_lm_\" + str(block_size))\n\n        logger.info(\"Creating features from dataset file at %s\", directory)\n        self.examples = []\n        for _, row in df.iterrows():\n            conv = construct_conv(row, tokenizer)\n            self.examples.append(conv)\n\n        logger.info(\"Saving features into cached file %s\", cached_features_file)\n        with open(cached_features_file, \"wb\") as handle:\n            pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, item):\n        return torch.tensor(self.examples[item], dtype=torch.long)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(\n        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n    )\n\n    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n    model.resize_token_embeddings(len(tokenizer))\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    logger.info(\"*** Running trainng, Num examples = %d, Num Epochs = %d ***\", len(train_dataset), args.num_train_epochs)\n\n    global_step, epochs_trained = 0, 0\n    tr_loss, logging_loss = 0.0, 0.0\n\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n    )\n    set_seed(args)  # Added here for reproducibility\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        logger.warning(\"Epoch done!\")\n        for step, batch in enumerate(epoch_iterator):\n            \n            inputs, labels = (batch, batch)\n            if inputs.shape[1] > 1024: continue\n            inputs = inputs.to(args.device)\n            labels = labels.to(args.device)\n            model.train()\n            outputs = model(inputs, labels=labels)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    logging_loss = tr_loss\n\n    tb_writer.close()\n\n    return global_step, tr_loss / global_step","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main(df_trn):\n    args = Args()\n    \n    # Setup CUDA, GPU & distributed training\n    device = torch.device(\"cuda\")\n    args.n_gpu = torch.cuda.device_count()\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n    )\n    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s\", args.local_rank, device, args.n_gpu)\n\n    set_seed(args) # Set seed\n\n    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, from_tf=False, config=config, cache_dir=args.cache_dir)\n    model.to(args.device)\n    \n    # Training\n    train_dataset = load_and_cache_examples(args, tokenizer, df_trn)\n    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n\n    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n    os.makedirs(args.output_dir, exist_ok=True)\n\n    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n    model_to_save = (model.module if hasattr(model, \"module\") else model)  # Take care of distributed/parallel training\n    model_to_save.save_pretrained(args.output_dir)\n    tokenizer.save_pretrained(args.output_dir)\n\n    # Good practice: save your training arguments together with the trained model\n    torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n\n    # Load a trained model and vocabulary that you have fine-tuned\n    model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n    model.to(args.device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Working with pretrained model","metadata":{}},{"cell_type":"code","source":"# code from main function\nimport torch\ndevice = torch.device(\"cuda\")\noutput_dir = 'output-small'\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(output_dir)\ntokenizer = AutoTokenizer.from_pretrained(output_dir)\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chat(model, tokenizer, trained=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}